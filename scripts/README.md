# Scripts Reference

This directory collects operational helpers for loading seed data and validating the
interview analysis pipeline. All commands assume you are running them from the repository
root with a working Python 3 environment, access to MongoDB, and environment variables
loaded from `backend/.env` unless noted otherwise.

## import_annotators_via_api.sh

- Purpose: push annotator details from a CSV export into the HTTP API (`/api/candidates`)
  so the dashboard can display them immediately.
- Prerequisites:
  - Backend API reachable at `http://localhost:8001` (override via `API_BASE`).
  - CSV export using the headers generated by the Google Form.
  - Optional defaults set through environment variables:
    - `API_BASE` (default `http://localhost:8001`)
    - `API_ENDPOINT` (default `${API_BASE}/api/candidates`)
    - `POSITION` (default `Language Tutor`)
    - `EXPERIENCE_YEARS` (default `0`)

**Usage**

```bash
./scripts/import_annotators_via_api.sh [path/to/export.csv]
```

Without an argument the script looks for
`Language Tutor â€“ AI Training Project (Responses) - Form Responses 1.csv` in the repo
root. Each row becomes a JSON payload that is POSTed to the API; the script prints an
`[OK]` or `[FAIL]` line per record and finishes with a processed/failed summary.

## import_annotator_profiles.py

- Purpose: import the same CSV directly into MongoDB by upserting the `users` collection.
- Prerequisites:
  - `backend/.env` defines `MONGO_URL` and `DB_NAME`; dependencies from `backend` are
    installed so `get_users_collection` works.
  - MongoDB running and reachable.

**Usage**

```bash
python scripts/import_annotator_profiles.py \
  --csv path/to/export.csv \
  --position "Language Tutor" \
  --experience-years 2
```

`--csv` defaults to the same Google Form export filename, `--position` and
`--experience-years` provide fallback values when the CSV omits them. The script
normalises language skills, builds a short bio and performs an upsert keyed by email,
reporting the number of inserted and updated records.

## import_candidates_csv.py

- Purpose: load the hard-coded `candidates_data` list into the `candidates` collection of
  a development database.
- Prerequisites:
  - MongoDB available on `mongodb://localhost:27017`.
  - The `motor` dependency installed (`pip install motor`).
  - Update `candidates_data` if you need different seed entries.

**Usage**

```bash
python scripts/import_candidates_csv.py
```

The script parses the first 50 entries in the embedded dataset, synthesises skills,
experience years and role titles based on the education level, and inserts them into
`test_database.candidates`. It prints counts for inserted records and the post-run total.

## seed_candidates.py

- Purpose: seed `test_database.candidates` with a small curated set of sample candidates.
- Prerequisites: same as `import_candidates_csv.py`.

**Usage**

```bash
python scripts/seed_candidates.py
```

If the collection already contains documents the script exits early to avoid creating
duplicates. Otherwise it inserts eight predefined profiles with bios, skills and ISO8601
timestamps.

## test_analysis.py

- Purpose: sanity-check the analysis pipeline against the most recent completed interview.
- Prerequisites:
  - `backend/.env` provides `MONGO_URL` and `DB_NAME`.
  - MongoDB contains at least one interview with `status: "completed"` and a non-empty
    transcript.
  - Required backend dependencies are installed so `services.analysis_service` can import.

**Usage**

```bash
python scripts/test_analysis.py
```

The script prints a transcript preview (first three entries), runs
`AnalysisService.analyze_interview` in the `"behavioral"` mode, shows the headline scores,
and confirms whether the resulting analysis document persisted back to MongoDB.

## test_full_analysis_flow.py

- Purpose: exercise the same analysis path exposed by the API, including persistence checks
  and a simulated page reload.
- Prerequisites: identical to `test_analysis.py`.

**Usage**

```bash
python scripts/test_full_analysis_flow.py
```

The script finds the latest completed interview, clears any previous analysis fields,
invokes `InterviewService.analyze_interview`, validates the stored analysis record, and
verifies that reloading the document returns the same results.

## verify_last_transcript.py

- Purpose: quickly inspect the transcript stored on the most recently created interview.
- Prerequisites: same database configuration as the other analysis scripts.

**Usage**

```bash
python scripts/verify_last_transcript.py
```

The script fetches the newest interview, prints its identifier and creation time, then
walks the transcript array so you can confirm that the ingestion pipeline saved each
utterance correctly.

